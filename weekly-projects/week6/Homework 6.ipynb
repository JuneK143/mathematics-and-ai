{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e918709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 6 (due 08/08/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444be24-24ae-496b-92a7-5cf8abc38e05",
   "metadata": {},
   "source": [
    "# Neural networks and computer vision\n",
    "\n",
    "### Objective\n",
    "In this week's project, you will learn to train, validate, and test a neural network. You will explore how inputs change through feature extraction in convolutional neural networks (CNNs), and you will interpret the trained filters by the network.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "You will use the MNIST dataset, a standard dataset of handwritten digits, which is widely used for training and testing image processing systems.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "The code example below demonstrates how to define, train, validate, and test a CNN. The training and test accuracy after each completed epoch are shown after a completed\n",
    "\n",
    "**1. Explore a working example**\n",
    "1. Open `example.ipynb` and read the code.\n",
    "2. Consult the pytorch documentation to learn what the arguments of the various employed pytorch functions mean.\n",
    "3. Run the code.\n",
    "4. Replace SGD with Adam in the training process. Then run the code again.\n",
    "5. Save the output figures that show training and validation accuracy as a function of the number of epochs in your file system.\n",
    "\n",
    "**2. Build a network**\n",
    "Create your own working example. (You are allowed to copy any amount of code from `example.ipynb`.) Your CNN should be different from the CNN in the working example in the following ways:\n",
    "1. The new CNN should have three convolutional layers instead of two. The first layer creates 32 channels. The second layer creates 64 channels, and the third layer creates 128 channels.\n",
    "2. The pooling layer after the third layer should not employ any padding.\n",
    "3. The last hidden layer should have 512 neurons.\n",
    "4. For all layers except the output layer, the activation function should be a ReLU (use `torch.nn.ReLU`).\n",
    "\n",
    "**3. Train and evaluate a neural network**\n",
    "1. Train the neural network that you have constructed in the previous step. How have the upgrades with respect to the CNN in `example.ipynb` affected the CNN's training time?\n",
    "2. Test the neural network. How have the upgrades with respect to the CNN in `example.ipynb` affected the CNN's validation accuracy?\n",
    "3. Identify the number $k$ of training epochs that gives you a good tradeoff between training time and validation accuracy.\n",
    "4. Run your code again using $k$ epochs during training. Time the training (e.g. using the python library `time`).\n",
    "\n",
    "**4. Model validation and model selection**\n",
    "1. Use the validation set approach to identify the best number $c$ of channels in the first convolutional layer (consider $c\\in\\{2,15\\}$).\n",
    "2. Update your neural network architecture so that the first convolutional layer has $c$ channels.\n",
    "\n",
    "**5. Visualizing feature extraction**\n",
    "1. Use the function `plot_mapped_features` to view an input image and the corresponding first channel of the hidden state for each feature-extraction layer (i.e., each convolution layer and each pooling layer).\n",
    "2. Update the function so that it shows all channels instead of just one.\n",
    "3. Comment on where you observe differences between the channels within a layer.\n",
    "\n",
    "**6. Visualizing and interpreting filters**\n",
    "1. Use the function `plot_filters` to view the trained filters of the first convolutional layer.\n",
    "2. Identify the filters that perform blurring, sharpening, or horizontal or vertical edge detection.\n",
    "\n",
    "**7. Comparison to logistic regression**\n",
    "1. Construct and run a pipeline for multiclass logistic regression of the MNIST dataset using sklearn.\n",
    "2. Comment on how the training time and test accuracy of logistic regression compare to the CNN.\n",
    "3. Now run multiclass logistic regression on the MNIST data set using one of the hidden states of the CNN (i.e., $\\vec{x}^{(2)}$, $\\vec{x}^{(3)}$, ..., $\\vec{x}^{(7)}$) as inputs. Which set of inputs yields the best classification results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e15d290-c0b4-4ec9-8db4-908d3c4d1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff8c25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "\n",
    "# Define an array transformation that transforms the images to tensor format \n",
    "# and normalizes the pixel values to the range [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training and test datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "    download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "    download=True, transform=transform)\n",
    "\n",
    "# Split the training dataset into a training set and a validation set\n",
    "train_set, val_set = random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Create data loaders for the training, validation, and test sets\n",
    "# A DataLoader in PyTorch is an object that simplifies and automates\n",
    "# batching, shuffling, and loading data for model training and evaluation. \n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79f96dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2. Update CNN\n",
    "\"\"\"\n",
    "# Define CNN architecture\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        \"\"\"\n",
    "        Initialize the CNN model by defining its layers.\n",
    "        \"\"\"\n",
    "        # Create an instance of the parent class `nn.Module`\n",
    "        super(CNN, self).__init__()  \n",
    "        self.c = c\n",
    "        self.conv1 = nn.Conv2d(1, c, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(c, c*2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(c*2, c*4, kernel_size=3, padding=1)\n",
    "        # Define the activation function\n",
    "        self.activation = nn.ReLU()\n",
    "        # Define a pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Inputs are num_channels in previous layer x image height x image width\n",
    "        self.fc1 = nn.Linear(c*4 * 3 * 3, 512)\n",
    "        # Define the output layer with 10 nodes\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the CNN.\n",
    "\n",
    "        Parameters:\n",
    "        x : torch.Tensor\n",
    "            The input tensor containing the image batch.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor\n",
    "            The output tensor containing the class scores for each image.\n",
    "        \"\"\"\n",
    "        # Pass the input through the first convolutional layer, then apply activation\n",
    "        x = self.activation(self.conv1(x))\n",
    "        # Pass the input through the first pooling layer\n",
    "        x = self.pool(x)\n",
    "        # Pass the input through the second convolutional layer, then apply activation\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # Pass the input through the second pooling layer\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = self.pool(x)     \n",
    "        # Change the shape of x into a 1d array\n",
    "        x = x.view(-1, self.c*4 * 3 * 3)\n",
    "        # Pass the input through the full connected hidden layer, then apply activation\n",
    "        x = self.activation(self.fc1(x))\n",
    "        # Pass the input through the last layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76a8a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training pipeline including validation after each epoch\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train the CNN model.\n",
    "\n",
    "    Parameters:\n",
    "    model : torch.nn.Module\n",
    "        The CNN model to be trained.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        The data loader for the training set.\n",
    "    val_loader : torch.utils.data.DataLoader\n",
    "        The data loader for the validation set.\n",
    "    criterion : torch.nn.modules.loss._Loss\n",
    "        The loss function to be used.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to be used.\n",
    "    epochs : int\n",
    "        The number of epochs for training.\n",
    "\n",
    "    Returns:\n",
    "    tuple\n",
    "        A tuple containing lists of training loss, validation loss, training accuracy, and validation accuracy.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store training and validation loss\n",
    "    train_loss, val_loss = [], []\n",
    "    # Initialize lists to store training and validation and accuracy\n",
    "    train_acc, val_acc = [], []\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()  \n",
    "        # Initialize the running loss for the epoch\n",
    "        running_loss = 0.0  \n",
    "        # Initialize counters for correct predictions and total samples\n",
    "        correct, total = 0, 0  \n",
    "\n",
    "        # Learning algorithm is SGD with minibatch. Iterating over the dataload\n",
    "        # returns images and labels in batches.\n",
    "        \n",
    "        # Iterate over batches of training data\n",
    "        for images, labels in train_loader:\n",
    "            # Zero the gradients to prevent accumulation from previous iterations\n",
    "            optimizer.zero_grad()  \n",
    "            # Perform a forward pass through the model to get predictions\n",
    "            outputs = model(images)  \n",
    "            # Compute the loss between predictions and true labels\n",
    "            loss = criterion(outputs, labels)  \n",
    "            # Perform a backward pass to compute gradients via backpropagation\n",
    "            loss.backward()  \n",
    "            # Update model parameters based on the computed gradients\n",
    "            optimizer.step()  \n",
    "\n",
    "            # Add up the loss\n",
    "            running_loss += loss.item()  \n",
    "            # Get the predicted class with the highest score\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            # Update the total number of samples\n",
    "            total += labels.size(0)  \n",
    "            # Update the number of correct predictions\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "\n",
    "        # Compute and store the average training loss for the epoch\n",
    "        train_loss.append(running_loss / len(train_loader))  \n",
    "        # Compute and store the training accuracy for the epoch\n",
    "        train_acc.append(100 * correct / total)  \n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()  \n",
    "        # Initialize the running loss for validation\n",
    "        val_running_loss = 0.0  \n",
    "        #  Initialize counters for correct predictions and total samples in validation\n",
    "        val_correct, val_total = 0, 0  \n",
    "        \n",
    "        # Disable gradient calculation for validation to save memory and computation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over batches of validation data\n",
    "            for images, labels in val_loader:\n",
    "                # Perform a forward pass through the model to get predictions\n",
    "                outputs = model(images)  \n",
    "                # Compute the loss between predictions and true labels\n",
    "                loss = criterion(outputs, labels)  \n",
    "                # Add up the loss\n",
    "                val_running_loss += loss.item()  \n",
    "                # Get the predicted class with the highest score\n",
    "                _, predicted = torch.max(outputs.data, 1)  \n",
    "                # Update the total number of samples in validation\n",
    "                val_total += labels.size(0)  \n",
    "                # Update the number of correct predictions in validation\n",
    "                val_correct += (predicted == labels).sum().item()  \n",
    "\n",
    "        # Compute and store the average validation loss for the epoch\n",
    "        val_loss.append(val_running_loss / len(val_loader))\n",
    "        # Compute and store the validation accuracy for the epoch\n",
    "        val_acc.append(100 * val_correct / val_total)  \n",
    "        \n",
    "        # Print the results for the current epoch, including training and validation loss and accuracy\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "              f'Validation Loss: {val_running_loss / len(val_loader):.4f}, '\n",
    "              f'Train Acc: {100 * correct / total:.2f}%, Val Acc: {100 * val_correct / val_total:.2f}%')\n",
    "        \n",
    "    # Return the lists of training and validation loss and accuracy\n",
    "    return train_loss, val_loss, train_acc, val_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6f961f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 2\n",
      "Epoch [1/10], Train Loss: 0.7015, Validation Loss: 0.2484, Train Acc: 78.19%, Val Acc: 92.49%\n",
      "Epoch [2/10], Train Loss: 0.1884, Validation Loss: 0.1677, Train Acc: 94.16%, Val Acc: 94.94%\n",
      "Epoch [3/10], Train Loss: 0.1426, Validation Loss: 0.1321, Train Acc: 95.56%, Val Acc: 95.65%\n",
      "Epoch [4/10], Train Loss: 0.1163, Validation Loss: 0.1143, Train Acc: 96.29%, Val Acc: 96.49%\n",
      "Epoch [5/10], Train Loss: 0.0995, Validation Loss: 0.0926, Train Acc: 96.90%, Val Acc: 97.23%\n",
      "Epoch [6/10], Train Loss: 0.0880, Validation Loss: 0.0909, Train Acc: 97.22%, Val Acc: 97.15%\n",
      "Epoch [7/10], Train Loss: 0.0815, Validation Loss: 0.0820, Train Acc: 97.44%, Val Acc: 97.39%\n",
      "Epoch [8/10], Train Loss: 0.0730, Validation Loss: 0.0857, Train Acc: 97.65%, Val Acc: 97.32%\n",
      "Epoch [9/10], Train Loss: 0.0682, Validation Loss: 0.0794, Train Acc: 97.84%, Val Acc: 97.46%\n",
      "Epoch [10/10], Train Loss: 0.0631, Validation Loss: 0.0766, Train Acc: 97.88%, Val Acc: 97.63%\n",
      "time to run was 129.76415270904545 s\n",
      "c = 4\n",
      "Epoch [1/10], Train Loss: 0.5007, Validation Loss: 0.1591, Train Acc: 84.86%, Val Acc: 95.38%\n",
      "Epoch [2/10], Train Loss: 0.1304, Validation Loss: 0.1017, Train Acc: 96.02%, Val Acc: 96.93%\n",
      "Epoch [3/10], Train Loss: 0.0917, Validation Loss: 0.0752, Train Acc: 97.17%, Val Acc: 97.73%\n",
      "Epoch [4/10], Train Loss: 0.0755, Validation Loss: 0.0813, Train Acc: 97.61%, Val Acc: 97.39%\n",
      "Epoch [5/10], Train Loss: 0.0622, Validation Loss: 0.0544, Train Acc: 98.06%, Val Acc: 98.26%\n",
      "Epoch [6/10], Train Loss: 0.0546, Validation Loss: 0.0573, Train Acc: 98.19%, Val Acc: 98.22%\n",
      "Epoch [7/10], Train Loss: 0.0464, Validation Loss: 0.0523, Train Acc: 98.57%, Val Acc: 98.43%\n",
      "Epoch [8/10], Train Loss: 0.0422, Validation Loss: 0.0454, Train Acc: 98.63%, Val Acc: 98.65%\n",
      "Epoch [9/10], Train Loss: 0.0343, Validation Loss: 0.0522, Train Acc: 98.90%, Val Acc: 98.49%\n",
      "Epoch [10/10], Train Loss: 0.0318, Validation Loss: 0.0519, Train Acc: 98.98%, Val Acc: 98.30%\n",
      "time to run was 152.75869420799427 s\n",
      "c = 6\n",
      "Epoch [1/10], Train Loss: 0.3878, Validation Loss: 0.1414, Train Acc: 88.46%, Val Acc: 95.68%\n",
      "Epoch [2/10], Train Loss: 0.0991, Validation Loss: 0.0814, Train Acc: 96.87%, Val Acc: 97.59%\n",
      "Epoch [3/10], Train Loss: 0.0701, Validation Loss: 0.0692, Train Acc: 97.82%, Val Acc: 97.92%\n",
      "Epoch [4/10], Train Loss: 0.0558, Validation Loss: 0.0557, Train Acc: 98.25%, Val Acc: 98.28%\n",
      "Epoch [5/10], Train Loss: 0.0458, Validation Loss: 0.0510, Train Acc: 98.54%, Val Acc: 98.46%\n",
      "Epoch [6/10], Train Loss: 0.0388, Validation Loss: 0.0461, Train Acc: 98.76%, Val Acc: 98.53%\n",
      "Epoch [7/10], Train Loss: 0.0330, Validation Loss: 0.0508, Train Acc: 98.93%, Val Acc: 98.37%\n",
      "Epoch [8/10], Train Loss: 0.0288, Validation Loss: 0.0468, Train Acc: 99.07%, Val Acc: 98.61%\n",
      "Epoch [9/10], Train Loss: 0.0265, Validation Loss: 0.0467, Train Acc: 99.16%, Val Acc: 98.68%\n",
      "Epoch [10/10], Train Loss: 0.0260, Validation Loss: 0.0496, Train Acc: 99.13%, Val Acc: 98.52%\n",
      "time to run was 175.47488370898645 s\n",
      "c = 8\n",
      "Epoch [1/10], Train Loss: 0.3460, Validation Loss: 0.0972, Train Acc: 89.67%, Val Acc: 96.87%\n",
      "Epoch [2/10], Train Loss: 0.0779, Validation Loss: 0.0571, Train Acc: 97.57%, Val Acc: 98.26%\n",
      "Epoch [3/10], Train Loss: 0.0543, Validation Loss: 0.0536, Train Acc: 98.25%, Val Acc: 98.32%\n",
      "Epoch [4/10], Train Loss: 0.0437, Validation Loss: 0.0467, Train Acc: 98.61%, Val Acc: 98.53%\n",
      "Epoch [5/10], Train Loss: 0.0343, Validation Loss: 0.0397, Train Acc: 98.89%, Val Acc: 98.81%\n",
      "Epoch [6/10], Train Loss: 0.0296, Validation Loss: 0.0424, Train Acc: 99.01%, Val Acc: 98.67%\n",
      "Epoch [7/10], Train Loss: 0.0242, Validation Loss: 0.0377, Train Acc: 99.17%, Val Acc: 98.84%\n",
      "Epoch [8/10], Train Loss: 0.0224, Validation Loss: 0.0550, Train Acc: 99.24%, Val Acc: 98.46%\n",
      "Epoch [9/10], Train Loss: 0.0191, Validation Loss: 0.0470, Train Acc: 99.34%, Val Acc: 98.56%\n",
      "Epoch [10/10], Train Loss: 0.0162, Validation Loss: 0.0337, Train Acc: 99.50%, Val Acc: 98.99%\n",
      "time to run was 214.49219795805402 s\n",
      "c = 10\n",
      "Epoch [1/10], Train Loss: 0.3375, Validation Loss: 0.1010, Train Acc: 90.01%, Val Acc: 96.94%\n",
      "Epoch [2/10], Train Loss: 0.0816, Validation Loss: 0.0708, Train Acc: 97.48%, Val Acc: 97.89%\n",
      "Epoch [3/10], Train Loss: 0.0570, Validation Loss: 0.0565, Train Acc: 98.15%, Val Acc: 98.34%\n",
      "Epoch [4/10], Train Loss: 0.0436, Validation Loss: 0.0481, Train Acc: 98.65%, Val Acc: 98.57%\n",
      "Epoch [5/10], Train Loss: 0.0369, Validation Loss: 0.0454, Train Acc: 98.81%, Val Acc: 98.68%\n",
      "Epoch [6/10], Train Loss: 0.0285, Validation Loss: 0.0440, Train Acc: 99.11%, Val Acc: 98.74%\n",
      "Epoch [7/10], Train Loss: 0.0252, Validation Loss: 0.0452, Train Acc: 99.15%, Val Acc: 98.67%\n",
      "Epoch [8/10], Train Loss: 0.0218, Validation Loss: 0.0396, Train Acc: 99.29%, Val Acc: 98.93%\n",
      "Epoch [9/10], Train Loss: 0.0169, Validation Loss: 0.0465, Train Acc: 99.42%, Val Acc: 98.65%\n",
      "Epoch [10/10], Train Loss: 0.0158, Validation Loss: 0.0457, Train Acc: 99.49%, Val Acc: 98.69%\n",
      "time to run was 255.89834925008472 s\n",
      "c = 12\n",
      "Epoch [1/10], Train Loss: 0.2891, Validation Loss: 0.0920, Train Acc: 91.43%, Val Acc: 96.94%\n",
      "Epoch [2/10], Train Loss: 0.0712, Validation Loss: 0.0872, Train Acc: 97.72%, Val Acc: 97.26%\n",
      "Epoch [3/10], Train Loss: 0.0495, Validation Loss: 0.0473, Train Acc: 98.44%, Val Acc: 98.59%\n",
      "Epoch [4/10], Train Loss: 0.0381, Validation Loss: 0.0420, Train Acc: 98.81%, Val Acc: 98.78%\n",
      "Epoch [5/10], Train Loss: 0.0337, Validation Loss: 0.0477, Train Acc: 98.90%, Val Acc: 98.58%\n",
      "Epoch [6/10], Train Loss: 0.0263, Validation Loss: 0.0379, Train Acc: 99.14%, Val Acc: 98.81%\n",
      "Epoch [7/10], Train Loss: 0.0215, Validation Loss: 0.0365, Train Acc: 99.31%, Val Acc: 99.00%\n",
      "Epoch [8/10], Train Loss: 0.0171, Validation Loss: 0.0376, Train Acc: 99.44%, Val Acc: 98.87%\n",
      "Epoch [9/10], Train Loss: 0.0160, Validation Loss: 0.0360, Train Acc: 99.47%, Val Acc: 99.01%\n",
      "Epoch [10/10], Train Loss: 0.0135, Validation Loss: 0.0522, Train Acc: 99.56%, Val Acc: 98.59%\n",
      "time to run was 265.7874000000302 s\n",
      "c = 14\n",
      "Epoch [1/10], Train Loss: 0.2764, Validation Loss: 0.0706, Train Acc: 91.50%, Val Acc: 97.91%\n",
      "Epoch [2/10], Train Loss: 0.0641, Validation Loss: 0.0591, Train Acc: 97.99%, Val Acc: 98.19%\n",
      "Epoch [3/10], Train Loss: 0.0443, Validation Loss: 0.0373, Train Acc: 98.64%, Val Acc: 98.86%\n",
      "Epoch [4/10], Train Loss: 0.0330, Validation Loss: 0.0378, Train Acc: 98.94%, Val Acc: 98.81%\n",
      "Epoch [5/10], Train Loss: 0.0268, Validation Loss: 0.0390, Train Acc: 99.15%, Val Acc: 98.80%\n",
      "Epoch [6/10], Train Loss: 0.0223, Validation Loss: 0.0362, Train Acc: 99.25%, Val Acc: 98.90%\n",
      "Epoch [7/10], Train Loss: 0.0172, Validation Loss: 0.0516, Train Acc: 99.42%, Val Acc: 98.47%\n",
      "Epoch [8/10], Train Loss: 0.0158, Validation Loss: 0.0442, Train Acc: 99.46%, Val Acc: 98.75%\n",
      "Epoch [9/10], Train Loss: 0.0140, Validation Loss: 0.0330, Train Acc: 99.54%, Val Acc: 98.98%\n",
      "Epoch [10/10], Train Loss: 0.0127, Validation Loss: 0.0333, Train Acc: 99.58%, Val Acc: 99.18%\n",
      "time to run was 389.7811765000224 s\n"
     ]
    }
   ],
   "source": [
    "# Build and train a model\n",
    "from time import perf_counter\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "c_list = [2,4,6,8,10,12,14]\n",
    "for c in c_list:\n",
    "    print(\"c =\", c)\n",
    "    start = perf_counter()\n",
    "    # Create model\n",
    "    model = CNN(c=c)\n",
    "\n",
    "    # Set loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set training algorithm\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train model\n",
    "    train_loss, val_loss, train_acc, val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, epochs = epochs)\n",
    "\n",
    "    end = perf_counter()\n",
    "    print(\"time to run was\", end - start, \"s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac749de",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "The upgrades to the CNN in example.ipynb have significantly increased the training time. Whereas with the Adam algorithm and the example CNN, the training time for all 10 iterations of training epochs took ~2.5 minutes, the updated CNN, also using Adam, ran for 10 minutes for the first five iterations of training epochs (1-5), after which I terminated the run.\n",
    "\n",
    "The validation accuracy is much higher for the upgraded CNN. \n",
    "The example CNN yielded the following accuracy rates:\n",
    "\n",
    "Epoch [1/10], Train Loss: 1.3627, Validation Loss: 0.4408, Train Acc: 56.16%, Val Acc: 87.99%\n",
    "Epoch [2/10], Train Loss: 0.3149, Validation Loss: 0.2286, Train Acc: 91.31%, Val Acc: 93.59%\n",
    "Epoch [3/10], Train Loss: 0.1970, Validation Loss: 0.1635, Train Acc: 94.37%, Val Acc: 95.32%\n",
    "Epoch [4/10], Train Loss: 0.1496, Validation Loss: 0.1317, Train Acc: 95.61%, Val Acc: 96.22%\n",
    "Epoch [5/10], Train Loss: 0.1213, Validation Loss: 0.1093, Train Acc: 96.42%, Val Acc: 96.94%\n",
    "Epoch [6/10], Train Loss: 0.1020, Validation Loss: 0.0973, Train Acc: 97.00%, Val Acc: 97.15%\n",
    "Epoch [7/10], Train Loss: 0.0890, Validation Loss: 0.0885, Train Acc: 97.31%, Val Acc: 97.40%\n",
    "Epoch [8/10], Train Loss: 0.0792, Validation Loss: 0.0805, Train Acc: 97.62%, Val Acc: 97.61%\n",
    "Epoch [9/10], Train Loss: 0.0708, Validation Loss: 0.0793, Train Acc: 97.94%, Val Acc: 97.60%\n",
    "Epoch [10/10], Train Loss: 0.0631, Validation Loss: 0.0686, Train Acc: 98.15%, Val Acc: 97.94%\n",
    "\n",
    "The upgraded CNN yielded the following accuracy rates for the first five epochs:\n",
    "\n",
    "Epoch [1/10], Train Loss: 0.2155, Validation Loss: 0.0613, Train Acc: 93.20%, Val Acc: 98.28%\n",
    "Epoch [2/10], Train Loss: 0.0524, Validation Loss: 0.0473, Train Acc: 98.37%, Val Acc: 98.53%\n",
    "Epoch [3/10], Train Loss: 0.0338, Validation Loss: 0.0478, Train Acc: 98.95%, Val Acc: 98.61%\n",
    "Epoch [4/10], Train Loss: 0.0252, Validation Loss: 0.0375, Train Acc: 99.18%, Val Acc: 98.94%\n",
    "Epoch [5/10], Train Loss: 0.0219, Validation Loss: 0.0346, Train Acc: 99.30%, Val Acc: 98.98%\n",
    "\n",
    "With just one epoch, the validation accuracy was higher (98.28%) than the validation accuracy after ten epochs with the original CNN (97.94%)\n",
    "\n",
    "Additionally, the testing accuracy also increased with the upgraded CNN. Whil the testing accuracy for the example CNN was 98.16%, the testing accuracy was 98.91% for the upgraded CNN; however, this increase was less impressive.\n",
    "\n",
    "I identified k = 2 as a good mark for tradeoff between training time and validation accuracy. Validation was around 98.5% while the training time was less than five minutes.\n",
    "This was my output:\n",
    "Epoch [1/2], Train Loss: 0.2160, Validation Loss: 0.0569, Train Acc: 93.21%, Val Acc: 98.26%\n",
    "Epoch [2/2], Train Loss: 0.0480, Validation Loss: 0.0514, Train Acc: 98.49%, Val Acc: 98.46%\n",
    "time to run was 292.9701649999479 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239c50d",
   "metadata": {},
   "source": [
    "Part 4 \\\n",
    "c = 2 \\\n",
    "Epoch [1/5], Train Loss: 0.6116, Validation Loss: 0.2242, Train Acc: 81.47%, Val Acc: 93.15% \\\n",
    "Epoch [2/5], Train Loss: 0.1760, Validation Loss: 0.1475, Train Acc: 94.60%, Val Acc: 95.36% \\\n",
    "Epoch [3/5], Train Loss: 0.1337, Validation Loss: 0.1341, Train Acc: 95.76%, Val Acc: 95.71%\n",
    "Epoch [4/5], Train Loss: 0.1091, Validation Loss: 0.0933, Train Acc: 96.46%, Val Acc: 97.05%\n",
    "Epoch [5/5], Train Loss: 0.0953, Validation Loss: 0.0889, Train Acc: 96.98%, Val Acc: 97.28%\n",
    "time to run was 63.234789874986745 s\n",
    "c = 4\n",
    "Epoch [1/5], Train Loss: 0.4578, Validation Loss: 0.1374, Train Acc: 86.65%, Val Acc: 95.72%\n",
    "Epoch [2/5], Train Loss: 0.1144, Validation Loss: 0.0888, Train Acc: 96.40%, Val Acc: 97.30%\n",
    "Epoch [3/5], Train Loss: 0.0845, Validation Loss: 0.0736, Train Acc: 97.40%, Val Acc: 97.83%\n",
    "Epoch [4/5], Train Loss: 0.0662, Validation Loss: 0.0737, Train Acc: 97.87%, Val Acc: 97.72%\n",
    "Epoch [5/5], Train Loss: 0.0577, Validation Loss: 0.0651, Train Acc: 98.15%, Val Acc: 98.01%\n",
    "time to run was 75.20662604202516 s\n",
    "c = 6\n",
    "Epoch [1/5], Train Loss: 0.4566, Validation Loss: 0.1385, Train Acc: 85.52%, Val Acc: 95.78%\n",
    "Epoch [2/5], Train Loss: 0.1029, Validation Loss: 0.0883, Train Acc: 96.74%, Val Acc: 97.30%\n",
    "Epoch [3/5], Train Loss: 0.0714, Validation Loss: 0.0832, Train Acc: 97.77%, Val Acc: 97.31%\n",
    "Epoch [4/5], Train Loss: 0.0575, Validation Loss: 0.0578, Train Acc: 98.21%, Val Acc: 98.30%\n",
    "Epoch [5/5], Train Loss: 0.0454, Validation Loss: 0.0548, Train Acc: 98.58%, Val Acc: 98.40%\n",
    "time to run was 89.28104862500913 s\n",
    "c = 8\n",
    "Epoch [1/5], Train Loss: 0.3842, Validation Loss: 0.1071, Train Acc: 88.15%, Val Acc: 96.75%\n",
    "Epoch [2/5], Train Loss: 0.0875, Validation Loss: 0.0738, Train Acc: 97.27%, Val Acc: 97.75%\n",
    "Epoch [3/5], Train Loss: 0.0634, Validation Loss: 0.0554, Train Acc: 97.99%, Val Acc: 98.15%\n",
    "Epoch [4/5], Train Loss: 0.0486, Validation Loss: 0.0709, Train Acc: 98.45%, Val Acc: 97.88%\n",
    "Epoch [5/5], Train Loss: 0.0410, Validation Loss: 0.0428, Train Acc: 98.65%, Val Acc: 98.80%\n",
    "time to run was 104.57447908399627 s\n",
    "c = 10\n",
    "Epoch [1/5], Train Loss: 0.3868, Validation Loss: 0.0922, Train Acc: 87.96%, Val Acc: 97.28%\n",
    "Epoch [2/5], Train Loss: 0.0802, Validation Loss: 0.0639, Train Acc: 97.58%, Val Acc: 98.11%\n",
    "Epoch [3/5], Train Loss: 0.0540, Validation Loss: 0.0539, Train Acc: 98.28%, Val Acc: 98.37%\n",
    "Epoch [4/5], Train Loss: 0.0437, Validation Loss: 0.0438, Train Acc: 98.62%, Val Acc: 98.65%\n",
    "Epoch [5/5], Train Loss: 0.0354, Validation Loss: 0.0581, Train Acc: 98.89%, Val Acc: 98.23%\n",
    "time to run was 118.20634833304211 s\n",
    "c = 12\n",
    "Epoch [1/5], Train Loss: 0.3251, Validation Loss: 0.0867, Train Acc: 90.08%, Val Acc: 97.31%\n",
    "Epoch [2/5], Train Loss: 0.0677, Validation Loss: 0.0487, Train Acc: 97.89%, Val Acc: 98.55%\n",
    "Epoch [3/5], Train Loss: 0.0469, Validation Loss: 0.0512, Train Acc: 98.52%, Val Acc: 98.34%\n",
    "Epoch [4/5], Train Loss: 0.0366, Validation Loss: 0.0411, Train Acc: 98.82%, Val Acc: 98.66%\n",
    "Epoch [5/5], Train Loss: 0.0279, Validation Loss: 0.0359, Train Acc: 99.08%, Val Acc: 98.90%\n",
    "time to run was 132.7757134580752 s\n",
    "c = 14\n",
    "Epoch [1/5], Train Loss: 0.2967, Validation Loss: 0.0792, Train Acc: 90.90%, Val Acc: 97.44%\n",
    "Epoch [2/5], Train Loss: 0.0662, Validation Loss: 0.0587, Train Acc: 97.88%, Val Acc: 98.18%\n",
    "Epoch [3/5], Train Loss: 0.0449, Validation Loss: 0.0413, Train Acc: 98.55%, Val Acc: 98.81%\n",
    "Epoch [4/5], Train Loss: 0.0373, Validation Loss: 0.0382, Train Acc: 98.81%, Val Acc: 98.91%\n",
    "Epoch [5/5], Train Loss: 0.0269, Validation Loss: 0.0354, Train Acc: 99.15%, Val Acc: 98.95%\n",
    "time to run was 198.1088390829973 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5772c922-4df1-4972-b704-f3e3d70982fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the feature maps produced by different layers for a given image\n",
    "def plot_mapped_features(model, image, layers):\n",
    "    '''Example usage: \n",
    "    \n",
    "    >>> examples = iter(test_loader)\n",
    "    >>> example_data, example_labels = next(examples) # get one batch from test set\n",
    "    >>> example_image = example_data[0]\n",
    "    >>> layers = [model.conv1, model.pool, model.conv2, model.pool]\n",
    "    >>> plot_mapped_features(model, example_image, layers)\n",
    "    \n",
    "    '''\n",
    "    # Add a batch dimension to the image tensor (from (channels, height, width) to (1, channels, height, width))\n",
    "    x = image.unsqueeze(0)\n",
    "    # Create a subplot with 1 row and len(layers) columns\n",
    "    fig, axes = plt.subplots(1, len(layers))\n",
    "    # Iterate over the specified layers\n",
    "    for i, layer in enumerate(layers):\n",
    "        # Pass the image through the current layer\n",
    "        x = layer(x)\n",
    "        # Detach the feature map from the computation graph and move it to CPU, then convert it to a NumPy array\n",
    "        # Visualize the first channel of the feature map\n",
    "        axes[i].imshow(x[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "        # Turn off the axis for a cleaner look\n",
    "        axes[i].axis('off')\n",
    "    # Display the feature maps\n",
    "    plt.show()\n",
    "    \n",
    "# Function to visualize the filters of a given convolutional layer\n",
    "def plot_filters(layer, n_filters=6):\n",
    "    '''Example usage: \n",
    "\n",
    "    >>> layer = model.conv1\n",
    "    >>> plot_filters(layer, n_filters=6)\n",
    "    \n",
    "    '''\n",
    "    # Clone the weights of the convolutional layer to avoid modifying the original weights\n",
    "    filters = layer.weight.data.clone()\n",
    "    # Normalize the filter values to the range [0, 1] for better visualization\n",
    "    filters = filters - filters.min()\n",
    "    filters = filters / filters.max()\n",
    "    # Select the first n_filters to visualize\n",
    "    filters = filters[:n_filters]\n",
    "    # Create a subplot with 1 row and n_filters columns\n",
    "    fig, axes = plt.subplots(1, n_filters)\n",
    "    # Iterate over the selected filters\n",
    "    for i, filter in enumerate(filters):\n",
    "        # Transpose the filter dimensions to (height, width, channels) for visualization\n",
    "        axes[i].imshow(np.transpose(filter, (1, 2, 0)))\n",
    "        # Turn off the axis for a cleaner look\n",
    "        axes[i].axis('off')\n",
    "    # Display the filters\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "286216a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.91%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "\n",
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * test_correct / test_total\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
